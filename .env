# ==== Neo4j ====
NEO4J_URI=bolt://localhost:7687
NEO4J_DATABASE=neo4j
NEO4J_USERNAME=neo4j
#NEO4J_PASSWORD=                # see .bashrc environment variable


# ==== AI Providers ====
EMBED_PROVIDER=LMSTUDIO
CHAT_PROVIDER=LMSTUDIO

# ==== Provider Endpoints ====
OLLAMA_URL=http://localhost:11434
VLLM_URL=http://localhost:8001

# ==== Provider Models ====
# Ollama
OLLAMA_EMBED_MODEL=bge-m3
OLLAMA_EMBED_DIM=1024
OLLAMA_CHAT_MODEL=mistral:7b

# vLLM
VLLM_EMBED_MODEL=BAAI/bge-m3
VLLM_EMBED_DIM=1024
VLLM_CHAT_MODEL=mistralai/Mistral-7B-Instruct-v0.2

# LM Studio
LMSTUDIO_URL=http://localhost:1234
LMSTUDIO_EMBED_MODEL=awantologi/bge-m3-Q4_K_M-GGUF
LMSTUDIO_EMBED_DIM=1024
LMSTUDIO_CHAT_MODEL=qwen2.5-math-1.5b-instruct


# ==== Source documents ====
SOURCE_DIR=/home/pjs/Documents/RAG_docs

# ==== API ====
API_PORT=8000


# ==== Reranker (optional, local cross-encoder via sentence-transformers) ====
RERANK_ENABLED=true
RERANK_MODEL=BAAI/bge-reranker-base

# ==== Chunking ====
# Target tokens per chunk
CHUNK_TOKENS=1000
# Overlap tokens between chunks
CHUNK_OVERLAP=150

# ==== LaTeX handling ====
LATEX_ENABLE=true              # keep verbatim LaTeX blocks
LATEX_FULLTEXT_INDEX=true      # create BM25 fulltext index over latex_raw
LATEX_QUERY_WEIGHT=0.3         # fusion weight for latex_vec (0..1)
PROSE_QUERY_WEIGHT=0.6         # fusion weight for vec_text (0..1)
BM25_LATEX_WEIGHT=0.1          # fusion weight for BM25 latex

# ==== Facts (optional) ====
FACTS_BIAS_WEIGHT=2.0          # score multiplier: (1 + weight)
FACTS_TOPK=5

# ==== Retrieval ====
K_PROSE=40
K_LATEX=40
TOPK_FINAL=8

# ==== Logging ====
LOG_LEVEL=INFO
LOG_DIR=./logs


